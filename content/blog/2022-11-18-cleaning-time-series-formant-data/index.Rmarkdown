---
title: Cleaning time-series formant data
author: Miao Zhang
date: '2022-11-18'
slug: cleaning-time-series-formant-data
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2022-11-18T21:33:57-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---


```{=html}
<style type="text/css">
  body{
  font-size: 14pt;
}
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Load the packages

First, we need to load the packages. In this tutorial, we will use the packages below.

```{r load packages, message = F, warning = F}
# If you don't have the packages downloaded, uncomment the first below and install download them first.
#install.packages("tidyverse", "imputeTS", "mgcv", "pracma")

# If you have installed the packages, load them.
library(mgcv) # GAM
library(pracma) # Signal smoothing
library(imputeTS) # Filling in NA values
library(tidyverse) # I tend to load tidyverse packages at last since it's used the most frequently and you don't want some of the functions are masked by other less important packages.
```

## 2. Read in the data set

I will use some data of Chinese diphthong formants from my dissertation. The data consist of several Mandarin Chinese native speakers producing monophthongs and diphthongs. Formant extraction was done by using my dynamic seeding script. 30 data points were extracted from each token of vowel. I will only show how to clean F1 data from female speakers in this tutorial.

```{r read in data, message = F}
(raw_chn <- read_csv("chn_frmt.csv") |>
  select(-Seg_num, -Syll, -Word, -F4, -F2, -F3, -COG, -sdev, -kurt, -skew, -t_m) |> # Drop columns that we don't need
  filter(t >= 2 & t <= 28 & Gender == "F") |> # In the beginning and the end of the vowel segment, the formant extraction tends to be innaccurate, so we drop the 
  mutate(t = (t-2)/26, # Change the time points to percentage) 
         Seg = factor(Seg, levels = c("i", "a", "u", "ai", "au", "ou")),
         Speaker = factor(Speaker),
         Gender = factor(Gender)))
```

## 3. Remove the measuring errors

There are values that aren't even outliers but just errors in formant extraction (e.g., extremely high F1 values for high vowels of which F1 should be low). We need to take those values out first. Let's plot the raw data to identify those errors.

```{r raw f1}
ggplot(raw_chn, aes(t, F1)) + # x axis: time, y axis: F1
  geom_line(aes(group = File_name), alpha = 0.1) + # for each token of vowels, plot the line
  scale_y_continuous(n.breaks = 6) +
  facet_wrap(. ~ Seg) +
  theme_light() +
  labs(title = "Raw F1", y = "Hz") +
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank(),
        axis.ticks.x = element_blank())
```

Another way to visually check the errors is to use histogram. But this is not recommended for plotting time-series data as it cannot display the temporal structure of the data (e.g., the formant excursions of vowel sequences).

```{r raw F1 hist}
ggplot(raw_chn, aes(F1)) +
  geom_histogram(aes(group = File_name)) +
  facet_wrap(. ~ Seg) +
  theme_light() +
  labs(title = "Raw F1", y = "Count") 
```

The abrupt bumps in the first figure are probably just errors of formant extraction (they are not even outliers). We can set formant ranges for different vowels to remove those errors.

```{r set the filtering bars}
# Create the filtering bars
(f1_b <- tibble(Seg = levels(raw_chn$Seg) |> fct_relevel("i", "a", "u", "ai", "au", "ou"),
               F1_upr = c(500, 1150, 550, 1050, 1000, 750),
               F1_lwr = c(230, 650, 240, 500, 490, 250)))
```

```{r append the filtering bar}
# Join this small dataset to the original raw data
(f_f1b <- left_join(raw_chn, f1_b, by = c("Seg"))) # Join the two datasets by matching the Seg column
```

We can check the filtering bars by visualization

```{r Check bars}
ggplot(f_f1b, aes(t, F1)) + # x axis: time, y axis: F1
  geom_line(aes(group = File_name), alpha = 0.1) + # for each token of vowels, plot the line
  geom_hline(aes(yintercept = F1_upr, color = "Red"), linetype = 1, linewidth=1) +
  geom_hline(aes(yintercept = F1_lwr, color = "Red"), linetype = 2, linewidth=1) +
  scale_y_continuous(n.breaks = 6) +
  facet_wrap(. ~ Seg) +
  theme_light() +
  labs(title = "Raw F1 with filtering bars", y = "Hz") +
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank(),
        axis.ticks.x = element_blank())
```

## 4. Remove the measuring errors

Now we can remove the errors.

```{r Filter out}
(f1_filt <- f_f1b |> 
  filter(F1 < F1_upr & F1 > F1_lwr))
```

However, there is a problem with this approach. When dropping data by using `filter()`, if the row has a data that doesn't meet the condition in any columns, the entire row is dropped. This might accidentally drop rows in which the data in other columns do not need to be dropped. For example, one row might have a F1 error, but its F2 value is fine. In such case, we only need to remove the F1 value but keep the F2. Henceforth we may end up losing a lot of data points using `filter()`. And in fact, we did. We lost about 1500 rows of data.

So an alternative and better approach is just to set those errors with extreme formant values to NA by using `ifelse()` or `if_else()` function.

```{r set to NA}
(f1_nafilt <- f_f1b |> 
  mutate(F1 = ifelse(F1 < F1_upr & F1 > F1_lwr, F1, NA)) |>
  select(-F1_upr, -F1_lwr))
```

Now we were able to keep all rows in the original data (the data sets before and after removal both contain 32338 rows of data).

Then we can visualize the data again.

```{r no error}
ggplot(f1_nafilt, aes(t, F1)) + # x axis: time, y axis: F1
  geom_line(aes(group = File_name), alpha = 0.1) + # for each token of vowels, plot the line
  scale_x_continuous(n.breaks = 3) +
  scale_y_continuous(n.breaks = 6) +
  facet_wrap(. ~ Seg) +
  theme_light() +
  labs(title = "Errors removed", y = "Hz") +
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank(),
        axis.ticks.x = element_blank())
```

## 5. Remove data points that are 2 sds away from the mean

Although we don't have extreme errors now, but the data still look very messy. So we can further filter out the data that are 2 standard deviations away from the mean in each time points for each vowel. This assumes that for each vowel on each time point, the data follow normal distribution. However, depending on how you want to analyze your own data, you may actually want to keep those data points that are 2 standard deviations away, or use 3 standard deviations instead of 2.

```{r filter out 2sds away}
(f1_clean <- f1_nafilt |> group_by(Seg, t) |> # Group the data by vowel segments and time
   mutate(F1 = ifelse(F1 > (mean(F1, na.rm = T) - 2*sd(F1, na.rm = T)) & 
                       F1 < (mean(F1, na.rm = T) + 2*sd(F1, na.rm = T)), F1, NA)) |>
   group_by(File_name))

# Let's visualize the filtered clean data
ggplot(f1_clean, aes(t, F1)) + # x axis: time, y axis: F1
  geom_line(aes(group = File_name), alpha = 0.1) + # for each token of vowels, plot the line
  scale_y_continuous(n.breaks = 6) +
  facet_wrap(. ~ Seg) +
  theme_light() +
  labs(title = "Filtered F1", y = "Hz") +
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank(),
        axis.ticks.x = element_blank())
```

The result looks much better.

However, you can see that R is telling us that it ignored 1594 rows of data that contain NA values. Also if a token has too many NAs, we probably should drop it from the data (for example, we probably don't need a token of vowel that over 40% of the data points are just NAs). And for data that contains only a few NAs, we want to fill (interpolate) the NAs based on the distribution of the adjacent data points.

Consider that I have a data set with 5 data points: 1, 2, NA, 4, 5. Even though, the third data point is an NA, we can be rather sure that it should be 3 according to the distribution of the overall data set. However, if we had a data set with 5 data points: 1, NA, NA, NA, 7, then we are not sure what the three missing values would be. Therefore, we should keep the first data set and fill the NA value with three but drop the second data set.

Therefore, next we will first drop all the tokens that have too many NAs, and then interpolate all the NAs left in other tokens.

```{r drop and interpolate NAs}
(f1_no_na <- group_by(f1_clean, File_name) |> # Make sure each group has only one token of vowels, in my dataset there is only one vowel in each recording sound files, so I only need to group it by File_name
  mutate(naf1 = sum(is.na(F1))) |> # calculate the NAs in each token
  filter(naf1 <= 8) |> # keep all tokens that have no more than 8 NAs
  select(-naf1) |> # drop the naf1 column we just created 
  mutate(F1 = na_interpolation(F1, option = "linear"))) # then use the na_interpolation() function from the package imputeTS to interpolate the NAs. The method is set to linear as I assume in each local context, the change of the data can be considered linear. There are other interpolating method in this function as well.
```

It looks we lost 121 (1285-1406) tokens of vowels, which is fine as it's only 8.6% of our original dataset. 

Let's also check how many NAs there are. There should be no NAs in the filtered data set now.

```{r check the number of NAs}
sum(is.na(f1_no_na$F1))
```

Visualize the interpolated data:

```{r visualize no na}
ggplot(f1_no_na, aes(t, F1)) + # x axis: time, y axis: F1
  geom_line(aes(group = File_name), alpha = 0.1) + # for each token of vowels, plot the line
  scale_x_continuous(n.breaks = 3) +
  scale_y_continuous(n.breaks = 6) +
  facet_wrap(. ~ Seg) +
  theme_light() +
  labs(title = "NAs interpolated", y = "Hz") +
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank(),
        axis.ticks.x = element_blank())
```

## 6. Smoothing the data by using movine average smoothing and GAM

However, this data is still messy containing lots of abrupt jumps of F1. We will need to smooth the data by using smoothing methods. There are several possible options: ss-anova, polynomial regression, generalized additive modeling, and lowess.

In the field of speech signal processing, the most common way is to use some kind of smoothing functions first and then smooth the data by using more sophisticated models I listed above.

My approach is to use moving average smoothing to smooth the data first and the smooth the entire curve by using GAM.

```{r smoothing}
f1_nest <- group_by(f1_no_na, File_name) |> nest()

for (i in (1:nrow(f1_nest))) {
  f1_pred_i <- f1_nest[i,] |> unnest(cols = c(data)) 
  f1_pred_i <- f1_pred_i |>
    mutate(smth_f1 = pracma::movavg(F1, n = 5, type = "t"), # we take 5 data points as the width of the smoothing angle, the method is set to "t", which represents "triangular"
           gam_f1 = predict(gam(smth_f1 ~ s(t, k = 3, bs = "tp", m = 1), data = f1_pred_i),
                            f1_pred_i))
  
  if (i == 1) {
    f1_pred <- f1_pred_i
  } else {
    f1_pred <- bind_rows(f1_pred, f1_pred_i)
  }
}
```

Then we can finally see the output of our data cleaning.

```{r final visualization}
ggplot(f1_pred, aes(t, gam_f1)) + # x axis: time, y axis: gam_f1
  geom_line(aes(group = File_name), alpha = 0.1) + # for each token of vowels, plot the line
  scale_x_continuous(n.breaks = 3) +
  scale_y_continuous(n.breaks = 6) +
  facet_wrap(. ~ Seg) +
  theme_light() +
  labs(title = "Smoothed F1", y = "Hz") +
  theme(axis.text.x = element_blank(),
        axis.title.x = element_blank(),
        axis.ticks.x = element_blank())
```

The final result looks much improved than the raw data.

